{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9dd764961984d7",
   "metadata": {},
   "source": [
    "场景1、入门使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T11:48:35.509289Z",
     "start_time": "2025-12-24T11:48:35.502715Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好，我是人类\\nAI: 你好，我是ai助手\\nHuman: 很开心认识你\\nAI: 我也是'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory=ConversationBufferMemory()#注意这里return_messages默认为false\n",
    "#将消息保存到内存中\n",
    "#不管inputs、outputs的key用什么名字，都认为inputs的key是human，outputs的key是AI\n",
    "memory.save_context(inputs={\"input\":\"你好，我是人类\"},outputs={\"outputs\":\"你好，我是ai助手\"})\n",
    "memory.save_context(inputs={\"input\":\"很开心认识你\"},outputs={\"outputs\":\"我也是\"})\n",
    "# print(memory.load_memory_variables())\n",
    "# 缺少大括号会报错#TypeError: load_memory_variables() missing 1 required positional argument: 'inputs'\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818055fdfba4dda",
   "metadata": {},
   "source": [
    "场景2：结合chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c215d6639e1246df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '你好,我叫Tom', 'history': '', 'text': '你好，Tom！很高兴和你聊天。有什么我可以帮助你的吗？'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "你可以与人类对话，\n",
    "历史：{history}，\n",
    "当前问题：{question}，\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "result=chain.invoke({\"question\": \"你好,我叫Tom\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0fca07",
   "metadata": {},
   "source": [
    "注意：上面的提示词模板中，{history}是必须的，否则会报错。因为memory默认会将历史记录存储在{history}中。由于memory的history为空，所以打印结果history为空(一对空的单引号)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba814ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '我的名字是什么？', 'history': 'Human: 你好,我叫Tom\\nAI: 你好，Tom！很高兴和你聊天。有什么我可以帮助你的吗？', 'text': '你的名字是Tom。很高兴认识你！有什么我可以帮助你的吗？'}\n"
     ]
    }
   ],
   "source": [
    "res=chain.invoke({\"question\": \"我的名字是什么？\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e78e07",
   "metadata": {},
   "source": [
    "可以通过memory_key修改memory数据的变量名，默认是history。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c37b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '我叫Tom', 'chat_history': '', 'text': '你好，Tom！很高兴认识你。有什么我可以帮助你的吗？'}\n",
      "{'question': '我的名字是什么？', 'chat_history': 'Human: 我叫Tom\\nAI: 你好，Tom！很高兴认识你。有什么我可以帮助你的吗？', 'text': '你的名字是Tom。很高兴再次见到你，Tom！有什么我可以帮你的吗？'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "template=\"\"\"\n",
    "你可以与用户进行对话。\n",
    "历史：{chat_history}\n",
    "用户问题：{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "chain = LLMChain(llm=model, prompt=prompt, memory=memory)\n",
    "\n",
    "res=chain.invoke({\"question\": \"我叫Tom\"})\n",
    "print(res)\n",
    "\n",
    "res=chain.invoke({\"question\": \"我的名字是什么？\"})\n",
    "print(res)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b82b5",
   "metadata": {},
   "source": [
    "使用ChatPromptTemplate和return_messages=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15129ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '中国的首都在哪里？', 'history': [HumanMessage(content='中国的首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国的首都位于北京。', additional_kwargs={}, response_metadata={})], 'text': '中国的首都位于北京。'}\n",
      "{'question': '我刚刚问了什么问题？', 'history': [HumanMessage(content='中国的首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国的首都位于北京。', additional_kwargs={}, response_metadata={}), HumanMessage(content='我刚刚问了什么问题？', additional_kwargs={}, response_metadata={}), AIMessage(content='您刚刚问了“中国的首都在哪里？”这个问题。', additional_kwargs={}, response_metadata={})], 'text': '您刚刚问了“中国的首都在哪里？”这个问题。'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt, memory=memory)\n",
    "result=chain.invoke({\"question\": \"中国的首都在哪里？\"})\n",
    "print(result)\n",
    "\n",
    "result=chain.invoke({\"question\": \"我刚刚问了什么问题？\"})\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
